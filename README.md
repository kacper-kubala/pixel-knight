# Pixel Knight

A cyberpunk-styled AI chat interface with support for multiple AI providers, web search, RAG, and advanced features.

## Features

### Core Features
- ğŸ”Œ **Multi-Provider Support** - Use OpenAI, Anthropic, Groq, xAI (Grok), Ollama, and more simultaneously
- ğŸ¤– **Multiple AI Models** - Switch between models from different providers
- ğŸ” **Web Search Integration** - Brave Search, DuckDuckGo, SearXNG
- ğŸ“š **RAG Support** - Index local documents for context-aware responses
- ğŸ’¾ **Session Management** - Save and organize your conversations
- ğŸ¨ **Cyberpunk UI** - Beautiful terminal-inspired dark theme
- âš¡ **Streaming Responses** - Real-time message streaming

### Advanced Features
- ğŸ”¬ **Deep Research Agent** - Multi-round research with synthesis
- ğŸ“º **YouTube Summarization** - Summarize videos automatically
- ğŸ¨ **Image Generation** - DALL-E integration for creating images
- ğŸ¤ **Voice Input** - Speech-to-text using Web Speech API
- âš–ï¸ **Model Comparison** - Compare responses from different models side-by-side
- ğŸ“ **Presets** - 10+ built-in system prompt presets for different use cases
- âœ¨ **Markdown Rendering** - Beautiful code highlighting with highlight.js

### UX Features
- âŒ¨ï¸ **Keyboard Shortcuts** - Ctrl+Enter, Ctrl+N, Escape, and more
- ğŸ“¥ **Export Conversations** - Export as Markdown or JSON
- âœï¸ **Edit Messages** - Edit your messages inline
- ğŸ” **Search History** - Search across all your conversations
- ğŸ”„ **Regenerate Responses** - Regenerate any AI response

### Infrastructure
- ğŸ³ **Docker Support** - Ready-to-use Dockerfile and docker-compose
- ğŸ˜ **PostgreSQL Support** - Optional database backend (SQLite fallback)
- ğŸ§ª **Tests** - pytest test suite with coverage
- ğŸš€ **CI/CD** - GitHub Actions workflow

## Quick Start

### One-Command Startup

```bash
./start.sh
```

That's it! The script will:
1. Create a virtual environment (if needed)
2. Install all dependencies (if needed)
3. Start the server

Then open **http://localhost:8080** in your browser.

### Manual Installation

If you prefer to run manually:

```bash
# Create virtual environment
python3 -m venv venv
source venv/bin/activate

# Install dependencies
pip install -r requirements.txt

# Run the server
python run.py
```

### Docker Deployment

```bash
# Run with Docker Compose (includes PostgreSQL)
docker-compose up -d

# Run with Ollama support
docker-compose --profile with-ollama up -d

# Just build and run the app
docker build -t pixel-knight .
docker run -p 8080:8080 pixel-knight
```

---

## ğŸ”Œ Adding AI Providers

Pixel Knight supports **multiple AI providers simultaneously**. Add providers in the UI through **Settings â†’ API Providers**.

### Supported Providers

| Provider | Models | Free Tier | API Key Required |
|----------|--------|-----------|------------------|
| ğŸ¦™ **Ollama** | Llama, Mistral, Qwen, Phi, etc. | âœ… Local | âŒ |
| ğŸ¤– **OpenAI** | GPT-4o, GPT-4, GPT-3.5-turbo | âŒ | âœ… |
| ğŸ§  **Anthropic** | Claude 3.5 Sonnet, Claude 3 Opus/Haiku | âŒ | âœ… |
| âš¡ **Groq** | Llama 3, Mixtral (ultra-fast) | âœ… | âœ… |
| ğ• **xAI** | Grok-2, Grok-beta | âŒ | âœ… |
| ğŸ¤ **Together AI** | 100+ open models | âœ… $25 credit | âœ… |
| ğŸŒ€ **Mistral AI** | Mistral Large, Medium, Small | âœ… | âœ… |
| ğŸ›¤ï¸ **OpenRouter** | 200+ models, one API | Pay per use | âœ… |

### Adding a Provider (UI)

1. Click **âš™ï¸ Settings** in the right sidebar
2. In the **API Providers** modal, click on a provider button (e.g., OpenAI, Groq)
3. Enter your API key when prompted
4. Click **Test** to verify connection
5. Models from that provider will now appear in the sidebar!

### Provider Details

#### ğŸ¦™ Ollama (Local - Recommended for Starting)

[Ollama](https://ollama.ai/) runs models locally on your machine.

```bash
# Install Ollama
brew install ollama        # macOS
curl -fsSL https://ollama.ai/install.sh | sh   # Linux

# Download models
ollama pull llama3.2
ollama pull mistral
ollama pull codellama
ollama pull qwen2.5

# Start server
ollama serve
```

**Configuration:**
- API Base: `http://localhost:11434/v1`
- No API key needed
- Pixel Knight adds Ollama by default!

#### ğŸ¤– OpenAI

Get your API key from: https://platform.openai.com/api-keys

**Models:** `gpt-4o`, `gpt-4o-mini`, `gpt-4-turbo`, `gpt-3.5-turbo`, `o1-preview`, `o1-mini`

#### ğŸ§  Anthropic (Claude)

Get your API key from: https://console.anthropic.com/

**Models:** `claude-3-5-sonnet-20241022`, `claude-3-opus-20240229`, `claude-3-sonnet-20240229`, `claude-3-haiku-20240307`

#### âš¡ Groq (Ultra-Fast)

Get your API key from: https://console.groq.com/ (free tier available!)

**Models:** `llama-3.2-90b-vision-preview`, `llama-3.1-70b-versatile`, `mixtral-8x7b-32768`

#### ğ• xAI (Grok)

Get your API key from: https://console.x.ai/

**Models:** `grok-2`, `grok-beta`

#### ğŸ¤ Together AI

Get your API key from: https://api.together.xyz/ ($25 free credit)

**Models:** 100+ open-source models including Llama, Mixtral, Qwen, CodeLlama, etc.

#### ğŸŒ€ Mistral AI

Get your API key from: https://console.mistral.ai/

**Models:** `mistral-large-latest`, `mistral-medium-latest`, `mistral-small-latest`, `open-mixtral-8x22b`

#### ğŸ›¤ï¸ OpenRouter

Get your API key from: https://openrouter.ai/keys

**Access 200+ models** from OpenAI, Anthropic, Google, Meta, and more with a single API key!

### Adding a Custom Provider

For any OpenAI-compatible API:

1. Click **âš™ï¸ Settings**
2. Click **Custom** button
3. Enter:
   - **Name:** Display name (e.g., "My vLLM Server")
   - **API Base URL:** Your API endpoint (e.g., `http://localhost:8000/v1`)
   - **API Key:** If required

### Managing Providers

In the API Providers modal:
- **Test** - Verify connection and fetch available models
- **ON/OFF** - Enable or disable a provider (disabled providers don't show models)
- **Edit** - Change API key or URL
- **Delete** - Remove a provider

---

## Configuration

### Environment Variables

Create a `.env` file in the project root:

```env
# Default LLM API (fallback)
OPENAI_API_BASE=http://localhost:11434/v1
OPENAI_API_KEY=sk-no-key-required

# Search Providers
BRAVE_API_KEY=your-brave-api-key
SEARXNG_URL=http://localhost:8888

# Image Generation (DALL-E)
OPENAI_DALLE_KEY=sk-your-openai-key

# Database (optional - defaults to SQLite)
DATABASE_URL=postgresql+asyncpg://user:pass@host:5432/dbname

# Server
HOST=0.0.0.0
PORT=8080
```

> **Note:** Provider API keys can also be added through the UI (Settings â†’ API Providers), which stores them locally.

---

## â˜ï¸ Cloud Database (Supabase)

By default, Pixel Knight stores data in local JSON files. To sync data across multiple devices, use a cloud database like **Supabase** (free tier available).

### Step 1: Create Supabase Account

1. Go to **https://supabase.com** â†’ Sign up with GitHub
2. Click **New Project**
3. Set:
   - **Name:** `pixel-knight`
   - **Database Password:** generate a strong password (save it!)
   - **Region:** choose closest to you
4. Click **Create new project** (wait ~2 minutes)

### Step 2: Get Connection String

1. In your project â†’ **Settings** (gear icon) â†’ **Database**
2. Scroll to **Connection string** â†’ **URI** tab
3. Copy the connection string

### Step 3: Configure Pixel Knight

Create `.env` file:

```env
# Supabase Database (replace with your values)
DATABASE_URL=postgresql+asyncpg://postgres.[PROJECT-ID]:[PASSWORD]@aws-0-eu-central-1.pooler.supabase.com:6543/postgres

# LLM API
OPENAI_API_BASE=http://localhost:11434/v1
OPENAI_API_KEY=sk-no-key-required
```

**Important:** Change `postgresql://` to `postgresql+asyncpg://` in the URL!

### Step 4: Use on Multiple Devices

Copy the same `.env` file to each device:

```bash
# Device 1
./start.sh
# Console shows: "Using PostgreSQL database for sessions"

# Device 2 (same .env file)
./start.sh
# Same database, synced data! ğŸ‰
```

### Alternative Cloud Databases

| Provider | Free Tier | Link |
|----------|-----------|------|
| **Supabase** | 500 MB | https://supabase.com |
| **Neon** | 512 MB | https://neon.tech |
| **Railway** | $5 credit | https://railway.app |
| **Render** | 256 MB | https://render.com |

All use the same connection string format - just update `DATABASE_URL` in `.env`.

---

## Usage

### Creating a Session

1. Click **+** or **+ Chat** next to a model
2. Enter a session name (or leave blank for auto-naming)
3. Optionally adjust **temperature**, **max tokens**, and **system prompt**
4. Start chatting!

### LLM Parameters

Click **âš™ Parameters** to adjust:
- **Temperature** (0-2): Higher = more creative, Lower = more focused
- **Max Tokens**: Maximum response length
- **System Prompt**: Instructions for the AI

### Web Search

1. Click **ğŸ” Search Settings** â†’ choose provider
2. Click **Research** button to enable search for next message
3. Your query will be enriched with web results

### Deep Research

1. Click **Deep** button
2. Enter your research query
3. Set number of iterations (1-10)
4. The agent will perform multiple search rounds and synthesize findings

### RAG (Document Context)

1. Toggle **RAG** switch in right sidebar
2. Either:
   - Enter directory path â†’ **Add**
   - Click **ğŸ“ Upload Files** to upload documents
3. Supported formats: `.txt`, `.md`, `.py`, `.js`, `.ts`, `.json`, `.pdf`
4. Your questions will use indexed documents as context

### YouTube Summarization

1. Click **YT** button
2. Paste YouTube video URL
3. Click **Summarize**

### Image Generation

1. Click **ğŸ¨** button (requires `OPENAI_DALLE_KEY` in .env)
2. Enter your image prompt
3. Choose size, quality, and style
4. Click **Generate**

### Voice Input

1. Click **ğŸ¤** button
2. Allow microphone access
3. Speak your message
4. Text appears in the input field

### Model Comparison

1. Click **âš–ï¸ Compare Models** in sidebar
2. Select two models
3. Enter a prompt
4. Click **Compare** to see side-by-side responses

### Keyboard Shortcuts

| Shortcut | Action |
|----------|--------|
| `Ctrl+Enter` | Send message |
| `Ctrl+N` | New session |
| `Ctrl+K` | Focus input |
| `Ctrl+B` | Toggle sidebar |
| `Escape` | Close modal |
| `1-9` | Switch to session # |
| `?` | Show all shortcuts |

### Export & Edit

- **Export MD/JSON**: Click buttons in chat header
- **Edit Message**: Hover over your message â†’ click âœï¸
- **Regenerate**: Hover over AI response â†’ click ğŸ”„
- **Copy**: Hover over any message â†’ click ğŸ“‹

---

## Project Structure

```
pixel_knight/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ main.py              # FastAPI app
â”‚   â”œâ”€â”€ config.py            # Settings
â”‚   â”œâ”€â”€ models.py            # Data models
â”‚   â”œâ”€â”€ database.py          # SQLAlchemy/PostgreSQL
â”‚   â””â”€â”€ services/
â”‚       â”œâ”€â”€ llm_service.py       # LLM integration
â”‚       â”œâ”€â”€ provider_service.py  # Multi-provider management
â”‚       â”œâ”€â”€ search_service.py    # Web search
â”‚       â”œâ”€â”€ rag_service.py       # Document indexing
â”‚       â”œâ”€â”€ session_service.py   # Session management
â”‚       â”œâ”€â”€ youtube_service.py   # YouTube processing
â”‚       â”œâ”€â”€ research_service.py  # Deep research agent
â”‚       â”œâ”€â”€ preset_service.py    # System prompts presets
â”‚       â””â”€â”€ image_service.py     # DALL-E integration
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ index.html           # UI
â”‚   â”œâ”€â”€ styles.css           # Cyberpunk theme
â”‚   â””â”€â”€ app.js               # Frontend logic
â”œâ”€â”€ tests/                   # pytest tests
â”‚   â”œâ”€â”€ conftest.py
â”‚   â”œâ”€â”€ test_api.py
â”‚   â””â”€â”€ test_services.py
â”œâ”€â”€ .github/workflows/       # CI/CD
â”‚   â””â”€â”€ ci.yml
â”œâ”€â”€ data/                    # Stored sessions, indexes, providers
â”œâ”€â”€ Dockerfile               # Docker build
â”œâ”€â”€ docker-compose.yml       # Docker stack
â”œâ”€â”€ start.sh                 # One-click startup
â”œâ”€â”€ run.py                   # Python entry point
â”œâ”€â”€ pytest.ini               # Test config
â””â”€â”€ requirements.txt
```

## API Endpoints

### Providers
| Endpoint | Method | Description |
|----------|--------|-------------|
| `/api/providers` | GET/POST | List or add providers |
| `/api/providers/presets` | GET | Available preset providers |
| `/api/providers/preset/{key}` | POST | Add preset provider |
| `/api/providers/{id}` | PUT/DELETE | Update or delete provider |
| `/api/providers/{id}/toggle` | POST | Enable/disable provider |
| `/api/providers/{id}/test` | POST | Test provider connection |

### Sessions
| Endpoint | Method | Description |
|----------|--------|-------------|
| `/api/sessions` | GET/POST | List or create sessions |
| `/api/sessions/search` | GET | Search sessions by content |
| `/api/sessions/{id}` | GET/PUT/DELETE | Manage specific session |
| `/api/sessions/{id}/auto-name` | POST | Auto-generate session name |
| `/api/sessions/{id}/export` | GET | Export session (MD/JSON) |
| `/api/sessions/{id}/messages/{id}` | PUT | Edit a message |

### Chat
| Endpoint | Method | Description |
|----------|--------|-------------|
| `/api/chat` | POST | Send message (blocking) |
| `/api/chat/stream` | POST | Send message (streaming) |
| `/api/chat/{id}/regenerate` | POST | Regenerate response |
| `/api/compare/chat` | POST | Compare model responses |

### Features
| Endpoint | Method | Description |
|----------|--------|-------------|
| `/api/models` | GET | List all models from enabled providers |
| `/api/rag/files` | GET | List indexed files |
| `/api/rag/index` | POST | Index directory |
| `/api/rag/upload` | POST | Upload file for RAG |
| `/api/youtube/summarize` | POST | Summarize YouTube video |
| `/api/research` | POST | Deep research agent |
| `/api/images/generate` | POST | Generate image (DALL-E) |
| `/api/images/status` | GET | Check image generation status |

### Configuration
| Endpoint | Method | Description |
|----------|--------|-------------|
| `/api/presets` | GET/POST | Manage system prompts |
| `/api/usage` | GET | Usage statistics |
| `/api/config` | GET/PUT | Configuration |

## Troubleshooting

### "No models available"
- Make sure at least one provider is enabled
- Click **Settings** â†’ test your providers
- For Ollama: make sure `ollama serve` is running

### Provider test fails
- Check if the API key is correct
- Verify the API endpoint is reachable
- Some providers may have rate limits

### Models not loading
- Click **Test** on the provider to refresh model list
- Check for firewall/network issues
- Verify the API is accessible: `curl http://localhost:11434/v1/models`

### Slow responses
- Try a smaller model (e.g., `phi3` instead of `llama3.2`)
- Use Groq for faster inference
- Reduce `max_tokens` parameter
- Use GPU acceleration if available

## License

MIT License
